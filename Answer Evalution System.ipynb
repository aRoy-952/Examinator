{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "import math\n",
    "import requests\n",
    "from werkzeug.wrappers import Request, Response\n",
    "from flask import Flask ,redirect ,url_for ,request , render_template, jsonify\n",
    "from flask_cors import CORS, cross_origin\n",
    "from gensim.models import KeyedVectors\n",
    "from functools import lru_cache\n",
    "from concurrent.futures import ThreadPoolExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://192.168.69.153:8888\n",
      "Press CTRL+C to quit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ans_list:['hello']\n",
      "ref_ans_list:['OS']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "192.168.69.153 - - [03/May/2024 13:51:15] \"POST /examinator HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10797829180955887\n",
      "Total marks: 0\n",
      "Individual marks: [0]\n",
      "ans_list:[\"Python is a versatile programming language known for its simplicity and flexibility. It is extensively used in web development, data science, machine learning, and automation. Python's rich ecosystem of libraries and frameworks empowers developers to build robust and scalable applications efficiently. Its intuitive syntax and readability contribute to its popularity among developers worldwide.\", \"Python is a widely-used programming language known for its simplicity and versatility. It is commonly used in web development, data analysis, and machine learning. While Python's syntax is straightforward, it can sometimes be challenging for beginners to grasp certain concepts.\", \"The sun is a star located at the center of our solar system. It is primarily composed of hydrogen and helium and is the primary source of light and energy for Earth. The sun's energy is generated through nuclear fusion reactions in its core, where hydrogen atoms combine to form helium, releasing vast amounts of energy in the process.\"]\n",
      "ref_ans_list:[\"Python is a versatile programming language known for its simplicity and flexibility. It is extensively used in web development, data science, machine learning, and automation. Python's rich ecosystem of libraries and frameworks empowers developers to build robust and scalable applications efficiently. Its intuitive syntax and readability contribute to its popularity among developers worldwide.\", \"Python is a versatile programming language known for its simplicity and flexibility. It is extensively used in web development, data science, machine learning, and automation. Python's rich ecosystem of libraries and frameworks empowers developers to build robust and scalable applications efficiently. Its intuitive syntax and readability contribute to its popularity among developers worldwide.\", \"Python is a versatile programming language known for its simplicity and flexibility. It is extensively used in web development, data science, machine learning, and automation. Python's rich ecosystem of libraries and frameworks empowers developers to build robust and scalable applications efficiently. Its intuitive syntax and readability contribute to its popularity among developers worldwide.\"]\n",
      "1.0\n",
      "0.8074705600738525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "192.168.69.153 - - [03/May/2024 13:52:10] \"POST /examinator HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5442286729812622\n",
      "Total marks: 17\n",
      "Individual marks: [10, 7, 0]\n",
      "ans_list:['']\n",
      "ref_ans_list:['answer']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "192.168.69.153 - - [03/May/2024 13:54:16] \"POST /examinator HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total marks: 0\n",
      "Individual marks: [0]\n"
     ]
    }
   ],
   "source": [
    "stopwords_path = r'stopwords-en.txt'\n",
    "with open(stopwords_path, 'r', encoding='utf-8') as fh:\n",
    "    stopwords = set(fh.read().split(\",\"))\n",
    "\n",
    "\n",
    "def GrammerChecker(answer):\n",
    "    req = requests.get(\"https://api.textgears.com/check.php?text=\" + answer + \"&key=JmcxHCCPZ7jfXLF6\")\n",
    "    no_of_errors = len(req.json()['errors'])\n",
    "\n",
    "    #print(no_of_errors)\n",
    "\n",
    "    if no_of_errors > 5 :\n",
    "        g = 0\n",
    "    else:\n",
    "        g = 1\n",
    "    return g\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Convert to lowercase and remove punctuation\n",
    "    text = text.lower()\n",
    "    text = ''.join(char for char in text if char.isalnum() or char.isspace())\n",
    "    return text\n",
    "def tokenize_and_remove_stopwords(text):\n",
    "    # Tokenize the text and remove stopwords\n",
    "    tokens = [token for token in text.split() if token not in stopwords]\n",
    "    return tokens\n",
    "    \n",
    "def parallel_keyword_matching(source_doc, target_docs):\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        results = list(executor.map(KeyWordmatching, [source_doc]*len(target_docs), target_docs))\n",
    "    return results\n",
    "\n",
    "#key Word matching\n",
    "def KeyWordmatching(X, Y_lst):\n",
    "    result = 0\n",
    "    X_list = tokenize_and_remove_stopwords(preprocess_text(X))\n",
    "    \n",
    "    for Y in Y_lst:\n",
    "        Y_list = tokenize_and_remove_stopwords(preprocess_text(Y))\n",
    "        intersection = set(X_list).intersection(Y_list)\n",
    "        cosine = len(intersection) / (len(X_list) * len(Y_list)) ** 0.5\n",
    "        result += cosine\n",
    "    \n",
    "    result /= len(Y_lst)  # Average cosine similarity across target documents\n",
    "    \n",
    "    kval = 0\n",
    "    if cosine > 0.9:\n",
    "        kval = 1\n",
    "    elif cosine > 0.8:\n",
    "        kval = 2\n",
    "    elif cosine > 0.6:\n",
    "        kval = 3\n",
    "    elif cosine > 0.4:\n",
    "        kval = 4\n",
    "    elif cosine > 0.2:\n",
    "        kval = 5\n",
    "    else:\n",
    "        kval = 6\n",
    "    return kval\n",
    "\n",
    "#length of string\n",
    "def CheckLength(client_answer):\n",
    "    \n",
    "    client_ans = len(client_answer.split())\n",
    "    #return client_ans\n",
    "    kval1 = 0\n",
    "    if client_ans > 50:\n",
    "        kval1 = 1\n",
    "    elif client_ans > 40:\n",
    "        kval1 = 2\n",
    "    elif client_ans > 30:\n",
    "        kval1 = 3\n",
    "    elif client_ans > 20:\n",
    "        kval1 = 4\n",
    "    elif client_ans > 10:\n",
    "        kval1 = 5\n",
    "    else:\n",
    "        kval1 = 6\n",
    "    return kval1\n",
    "\n",
    "#Synonym\n",
    "\n",
    "class DocSim:\n",
    "    def __init__(self, w2v_model, stopwords=None):\n",
    "        self.w2v_model = w2v_model\n",
    "        self.stopwords = stopwords if stopwords is not None else []\n",
    "        \n",
    "    @lru_cache(maxsize=None)\n",
    "    def vectorize(self, doc: str) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Identify the vector values for each word in the given document\n",
    "        :param doc:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        doc = doc.lower()\n",
    "        words = [w for w in doc.split(\" \") if w not in self.stopwords]\n",
    "        word_vecs = []\n",
    "        for word in words:\n",
    "            try:\n",
    "                vec = self.w2v_model[word]\n",
    "                word_vecs.append(vec)\n",
    "            except KeyError:\n",
    "                # Ignore, if the word doesn't exist in the vocabulary\n",
    "                pass\n",
    "\n",
    "        # Assuming that document vector is the mean of all the word vectors\n",
    "        # PS: There are other & better ways to do it.\n",
    "        vector = np.mean(word_vecs, axis=0)\n",
    "        return vector\n",
    "\n",
    "    def _cosine_sim(self, vecA, vecB):\n",
    "        \"\"\"Find the cosine similarity distance between two vectors.\"\"\"\n",
    "        csim = np.dot(vecA, vecB) / (np.linalg.norm(vecA) * np.linalg.norm(vecB))\n",
    "        if np.isnan(np.sum(csim)):\n",
    "            return 0\n",
    "        return csim\n",
    "\n",
    "    def calculate_similarity(self, source_doc, target_docs=None, threshold=0.6):\n",
    "        \"\"\"Calculates & returns similarity scores between given source document & all\n",
    "        the target documents.\"\"\"\n",
    "        if not target_docs:\n",
    "            return []\n",
    "\n",
    "        if isinstance(target_docs, str):\n",
    "            target_docs = [target_docs]\n",
    "\n",
    "        source_vec = self.vectorize(source_doc)\n",
    "        results = []\n",
    "        result=[]\n",
    "        for doc in target_docs:\n",
    "            target_vec = self.vectorize(doc)\n",
    "            sim_score = self._cosine_sim(source_vec, target_vec)\n",
    "            result.append(sim_score)\n",
    "#             if sim_score > threshold:\n",
    "#                 results.append({\"score\": sim_score, \"doc\": doc})\n",
    "#             # Sort results by score in desc order\n",
    "#         results.sort(key=lambda k: k[\"score\"], reverse=True)\n",
    "\n",
    "        return result\n",
    "\n",
    "\n",
    "\n",
    "app = Flask(__name__)\n",
    "CORS(app)\n",
    "email = \"null\"\n",
    "name=\"null\"\n",
    "roll=\"null\"\n",
    "\n",
    "@app.route('/')\n",
    "@cross_origin()\n",
    "def Base_qstn_paper_set():\n",
    "    return render_template('index.html')\n",
    "\n",
    "\n",
    "@app.route('/examinator', methods=['POST', 'GET'])\n",
    "def examinator():\n",
    "    if request.method == 'POST':\n",
    "        data = request.json\n",
    "        first_list = data.get('first', [])\n",
    "        second_list = data.get('second',[])\n",
    "        name = data.get('name')\n",
    "        roll = data.get('roll')\n",
    "        email = data.get('emailID')\n",
    "        print(f\"ans_list:{first_list}\")\n",
    "        print(f\"ref_ans_list:{second_list}\")\n",
    "        \n",
    "        googlenews_model_path = r'GoogleNews-vectors-negative300.bin'\n",
    "        stopwords_path = r'stopwords-en.txt'\n",
    "\n",
    "        model = KeyedVectors.load_word2vec_format(googlenews_model_path, binary=True)\n",
    "        with open(stopwords_path, 'r', encoding='utf-8') as fh:\n",
    "            stopwords = fh.read().split(\",\")\n",
    "        ds = DocSim(model, stopwords=stopwords)\n",
    "\n",
    "        def check(source_docs, target_docs):\n",
    "            marks = []\n",
    "            for source_doc, target_doc in zip(source_docs, target_docs):\n",
    "                if len(source_doc.strip()) == 0:\n",
    "                    marks.append(0)\n",
    "                    continue\n",
    "                sim_scores = ds.calculate_similarity(source_doc, [target_doc])\n",
    "                similarity_score = sum(sim_scores) / len(sim_scores)\n",
    "                # Penalize for irrelevant answers\n",
    "                if similarity_score < 0.6:\n",
    "                    marks.append(0)\n",
    "#                     similarity_score=similarity_score-1\n",
    "                    print(similarity_score)\n",
    "                    continue\n",
    "                print(similarity_score)\n",
    "                key_match = KeyWordmatching(source_doc, [target_doc])\n",
    "#                 print(\"key_match:\", key_match)\n",
    "                key_Error = GrammerChecker(source_doc)\n",
    "#                 print(\"key_Error:\", key_Error)\n",
    "                key_length = CheckLength(source_doc)\n",
    "#                 print(\"key_length:\", key_length)\n",
    "                marks1 = (similarity_score * 6) + (1 / key_match) + (2 * key_Error) + (1 / key_length)\n",
    "                marks.append(round(marks1))\n",
    "            return marks\n",
    "\n",
    "        individual_marks = check(first_list, second_list)\n",
    "        total_marks = sum(individual_marks)\n",
    "\n",
    "        response_data = {\n",
    "            \"total_marks\": total_marks,\n",
    "            \"individual_marks\": individual_marks\n",
    "        }\n",
    "\n",
    "        print(f\"Total marks: {total_marks}\")\n",
    "        print(f\"Individual marks: {individual_marks}\")\n",
    "\n",
    "        return jsonify(response_data)\n",
    "\n",
    "    return jsonify({\"error\": \"Method not allowed\"})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    from werkzeug.serving import run_simple\n",
    "    run_simple('192.168.69.153', 8888, app)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
